# ğŸ§® Hoja de Apuntes - ModelaciÃ³n NumÃ©rica (VersiÃ³n Extendida)

## ğŸ§© Unidad 1: Errores

### ğŸ”¹ Tipos de errores
- **Error absoluto:** \( e_a = |x - x_{real}| \) â†’ Diferencia entre el valor verdadero y el medido.
- **Error relativo:** \( e_r = \frac{e_a}{|x_{real}|} \) â†’ ProporciÃ³n del error respecto al valor real.
- **Error porcentual:** \( e_p = e_r \times 100 \) â†’ Error expresado en porcentaje.

### ğŸ”¹ PropagaciÃ³n de errores
| Caso | Error Inherente | Error Redondeo | Error DiscretizaciÃ³n | Error Final |
|:----:|:----------------:|:---------------:|:---------------------:|:-------------:|
| I | Nulo | Nulo | Nulo | Nulo |
| II | No nulo | Nulo | Nulo | Depende solo del problema numÃ©rico |
| III | Nulo | No nulo | Nulo | Depende del algoritmo |
| IV | Nulo | Nulo | No nulo | Error de discretizaciÃ³n (cuando P.M â‰  P.N) |

### ğŸ”¹ Cifras significativas
- Indican el nivel de precisiÃ³n del nÃºmero medido.
- Ejemplo: 0,020 â†’ 2 cifras significativas.

---

## âš™ï¸ Unidad 2: Ecuaciones no lineales

### ğŸ”¸ MÃ©todo de BisecciÃ³n
**Objetivo:** Encontrar una raÃ­z de \( f(x) = 0 \) en un intervalo [a, b].  
**CondiciÃ³n:** \( f(a) \times f(b) < 0 \)

**FÃ³rmula:**
\[ x_r = \frac{a + b}{2} \]

**Pasos:**
1. Calcular \( x_r \)
2. Evaluar \( f(x_r) \)
3. Si \( f(a)f(x_r) < 0 \Rightarrow b = x_r \), si no \( a = x_r \)
4. Repetir hasta que \( |b - a| < tol \)

**Convergencia:** lineal.

---

### ğŸ”¸ MÃ©todo de Regula-Falsi (Falsa posiciÃ³n)
**FÃ³rmula:**
\[ x_r = b - f(b) \frac{(a - b)}{f(a) - f(b)} \]

Usa el mismo criterio que BisecciÃ³n pero mejora el punto medio con una interpolaciÃ³n lineal.  
**Convergencia:** lineal (mÃ¡s rÃ¡pida que bisecciÃ³n).

---

### ğŸ”¸ MÃ©todo de Punto Fijo
**Reescritura:** \( x = g(x) \)

**IteraciÃ³n:** \( x_{k+1} = g(x_k) \)

**CondiciÃ³n de convergencia:** \( |g'(x)| < 1 \)

**Convergencia:** lineal.

---

### ğŸ”¸ MÃ©todo de Newton-Raphson
**FÃ³rmula:**
\[ x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)} \]

**Condiciones:**
- f y f' deben ser continuas.
- \( f'(x) \neq 0 \).

**Convergencia:** cuadrÃ¡tica.

**Significado de sÃ­mbolos:**
- \( x_k \): aproximaciÃ³n actual.
- \( f(x_k) \): valor de la funciÃ³n.
- \( f'(x_k) \): derivada de la funciÃ³n.

---

### ğŸ”¸ MÃ©todo de la Secante
**FÃ³rmula:**
\[ x_{k+1} = x_k - f(x_k) \frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})} \]

**Ventajas:**
- No requiere derivada.
- **Orden de convergencia:** â‰ˆ 1.618 (superlineal).

---

### ğŸ”¸ Newton-Raphson para raÃ­ces mÃºltiples
**FÃ³rmula:**
\[ x_{k+1} = x_k - \frac{f(x_k) f'(x_k)}{[f'(x_k)]^2 - f(x_k)f''(x_k)} \]

**Convergencia:** cuadrÃ¡tica para raÃ­ces simples, lineal para mÃºltiples.

---

### ğŸ”¸ Orden de convergencia
\[ \lim_{kâ†’âˆ} \frac{|x_{k+1} - r|}{|x_k - r|^p} = C \]
- \( p \): orden de convergencia (1 = lineal, 2 = cuadrÃ¡tica)
- \( C \): constante de convergencia.

---

## ğŸ“ˆ Unidad 3: Ajuste de Curvas e InterpolaciÃ³n

### ğŸ”¸ Ajuste lineal por cuadrados mÃ­nimos
Queremos ajustar una recta: \( y = a + bx \)

**FÃ³rmulas:**
\[ b = \frac{nÎ£xy - (Î£x)(Î£y)}{nÎ£x^2 - (Î£x)^2} \]
\[ a = \bar{y} - b\bar{x} \]

**Error cuadrÃ¡tico medio:**
\[ ECM = \frac{1}{n} Î£ (y_i - (a + bx_i))^2 \]

**InterpretaciÃ³n:** mide quÃ© tan bien el modelo ajusta los datos.

---

### ğŸ”¸ Ajustes cuadrÃ¡tico y cÃºbico
- CuadrÃ¡tico: \( y = a + bt + ct^2 \)
- CÃºbico: \( y = a + bt + ct^2 + dt^3 \)

Coeficientes: \( (A^TA)x = A^Ty \)  o con `np.polyfit(x, y, grado)`.

---

### ğŸ”¸ Ajuste exponencial
\[ y = e^{a - bt} \]
TransformaciÃ³n logarÃ­tmica:
\[ \ln(y) = a - bt \]
Se aplica regresiÃ³n lineal entre t y ln(y).

---

### ğŸ”¸ InterpolaciÃ³n PolinÃ³mica
Interpola todos los puntos exactos (pasa por cada uno).

#### Polinomio de Lagrange
\[ P(x) = Î£ y_i L_i(x), \quad L_i(x) = Î _{jâ‰ i} \frac{x - x_j}{x_i - x_j} \]

#### Polinomio de Newton (diferencias divididas)
\[ P(x) = a_0 + a_1(x - x_0) + a_2(x - x_0)(x - x_1) + ... \]

**Error:**
\[ E(x) = \frac{f^{(n+1)}(Î¾)}{(n+1)!} Î (x - x_i) \]

#### Nodos de Chebyshev
Minimizan oscilaciones (fenÃ³meno de Runge).
\[ x_i = \cos\left(\frac{2i+1}{2n+2}Ï€\right) \]

#### Polinomio de Hermite
Usa valores y derivadas conocidas para interpolar.

---

## ğŸ§® Unidad 5: DiferenciaciÃ³n e IntegraciÃ³n NumÃ©rica

### ğŸ”¸ DiferenciaciÃ³n NumÃ©rica
#### FÃ³rmulas
- Hacia adelante: \( f'(x) â‰ˆ \frac{f(x+h) - f(x)}{h} \)
- Hacia atrÃ¡s: \( f'(x) â‰ˆ \frac{f(x) - f(x-h)}{h} \)
- Centrada: \( f'(x) â‰ˆ \frac{f(x+h) - f(x-h)}{2h} \)

**Error de truncamiento:** O(h) o O(hÂ²).

---

### ğŸ”¸ IntegraciÃ³n NumÃ©rica
#### Regla del trapecio
\[ I = \frac{h}{2} [f(x_0) + 2Î£f(x_i) + f(x_n)] \]
**Error:** \( E_t = -\frac{(b-a)h^2}{12}f''(Î¾) \)

#### Simpson 1/3
\[ I = \frac{h}{3} [f(x_0) + 4f(x_1) + 2f(x_2) + ... + 4f(x_{n-1}) + f(x_n)] \]
**Error:** \( E_t = -\frac{(b-a)h^4}{180}f^{(4)}(Î¾) \)

#### Simpson 3/8
\[ I = \frac{3h}{8}[f(x_0) + 3f(x_1) + 3f(x_2) + 2f(x_3) + ... + f(x_n)] \]

#### ExtrapolaciÃ³n de Richardson
\[ R = T(h_2) + \frac{T(h_2) - T(h_1)}{(h_1/h_2)^p - 1} \]
Donde \( p \) es el orden del mÃ©todo.

---

âœ… **Consejos finales**
- Revisar continuidad de f(x) y f'(x) antes de Newton.
- Normalizar variables para estabilidad numÃ©rica.
- Usar tolerancia y error relativo como criterio de parada.
- Preferir nodos de Chebyshev al interpolar con muchos puntos.

---
ğŸ“˜ **Fin de la versiÃ³n extendida**

